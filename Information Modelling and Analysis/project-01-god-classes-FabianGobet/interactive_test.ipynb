{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import sys\n",
    "import javalang\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pandas as pd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_god_classes(path_to_directory: str) -> List[str]:\n",
    "    df = generate_class_method_count_df(path_to_directory)\n",
    "    df['is_god'] = df['number_of_methods'].apply(lambda x: x > df['number_of_methods'].mean() + 6*df['number_of_methods'].std())\n",
    "    return df[df['is_god']]\n",
    "\n",
    "def generate_class_method_count_df(path_to_directory: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(columns=['class_name', 'number_of_methods', 'path'])\n",
    "    for path_walk, _, files_walk in os.walk(path_to_directory):\n",
    "        for file in files_walk:\n",
    "            if file.endswith('.java'):\n",
    "                with open(os.path.join(path_walk, file), 'r') as f:\n",
    "                    tree = javalang.parse.parse(f.read())\n",
    "                for _, class_declaration in tree.filter(javalang.tree.ClassDeclaration):\n",
    "                    number_of_methods = len(class_declaration.methods)\n",
    "                    df.loc[len(df.index)] = [class_declaration.name, number_of_methods, os.path.join(path_walk, file).replace('\\\\', '/')]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_name</th>\n",
       "      <th>number_of_methods</th>\n",
       "      <th>path</th>\n",
       "      <th>is_god</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>CoreDocumentImpl</td>\n",
       "      <td>125</td>\n",
       "      <td>./resources/xerces2-j-src/org/apache/xerces/do...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>DTDGrammar</td>\n",
       "      <td>101</td>\n",
       "      <td>./resources/xerces2-j-src/org/apache/xerces/im...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>XSDHandler</td>\n",
       "      <td>118</td>\n",
       "      <td>./resources/xerces2-j-src/org/apache/xerces/im...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>XIncludeHandler</td>\n",
       "      <td>116</td>\n",
       "      <td>./resources/xerces2-j-src/org/apache/xerces/xi...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           class_name  number_of_methods  \\\n",
       "109  CoreDocumentImpl                125   \n",
       "216        DTDGrammar                101   \n",
       "476        XSDHandler                118   \n",
       "679   XIncludeHandler                116   \n",
       "\n",
       "                                                  path  is_god  \n",
       "109  ./resources/xerces2-j-src/org/apache/xerces/do...    True  \n",
       "216  ./resources/xerces2-j-src/org/apache/xerces/im...    True  \n",
       "476  ./resources/xerces2-j-src/org/apache/xerces/im...    True  \n",
       "679  ./resources/xerces2-j-src/org/apache/xerces/xi...    True  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./resources/xerces2-j-src\"\n",
    "god_df = get_god_classes(path)\n",
    "god_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "god_df.drop(columns=['is_god']).to_csv('./generated/god_classes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fields(class_declaration: javalang.tree.ClassDeclaration) -> set[str]:\n",
    "    set_fields = set()\n",
    "    for m in class_declaration.fields:\n",
    "        set_fields.add(m.declarators[0].name)\n",
    "    return set_fields\n",
    "\n",
    "def get_methods(class_declaration: javalang.tree.ClassDeclaration) -> set[str]:\n",
    "    set_methods = set()\n",
    "    for m in class_declaration.methods:\n",
    "        set_methods.add(m.name)\n",
    "    return set_methods\n",
    "\n",
    "def get_fields_accessed_by_method(method_declaration: javalang.tree.MethodDeclaration) -> set[str]:\n",
    "    set_field_accesses = set()\n",
    "    for _,p in method_declaration.filter(javalang.tree.MemberReference):\n",
    "        set_field_accesses.add(p.qualifier if p.qualifier != '' else p.member)\n",
    "    return set_field_accesses\n",
    "\n",
    "def get_methods_accessed_by_method(method_declaration: javalang.tree.MethodDeclaration) -> set[str]:\n",
    "    set_method_accesses = set()\n",
    "    for _,p in method_declaration.filter(javalang.tree.MethodInvocation):\n",
    "        set_method_accesses.add(p.member)\n",
    "    return set_method_accesses\n",
    "\n",
    "\n",
    "def generate_feature_dataframe(node: javalang.tree.ClassDeclaration, set_class_methods: set, set_class_fields: set) -> pd.DataFrame:\n",
    "    features = set()\n",
    "    features.update(set_class_fields)\n",
    "    features.update(set_class_methods)\n",
    "    features = list(features)\n",
    "    df = pd.DataFrame(columns=['method_name']+features)\n",
    "    for m in list(set_class_methods):\n",
    "        df.loc[len(df)] = {'method_name': m}\n",
    "    for m in node.methods:\n",
    "        method_name = m.name\n",
    "        method_features = set()\n",
    "        method_features = method_features.union(get_fields_accessed_by_method(m))\n",
    "        method_features = method_features.union(get_methods_accessed_by_method(m))\n",
    "        for f in list(method_features):\n",
    "            if f in features:\n",
    "                if not df['method_name'].isin([method_name]).any():\n",
    "                    df.loc[len(df)] = {'method_name': method_name}\n",
    "                df.loc[df['method_name'] == method_name, f] = 1\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_feature_vectors(path_java_file: str, save_directory_path: str = './') -> pd.DataFrame:\n",
    "    with open(path_java_file, 'r') as f:\n",
    "        tree = javalang.parse.parse(f.read())\n",
    "    class_name = path_java_file.split('/')[-1].split('.')[0]\n",
    "    class_features = {}\n",
    "    for _,n in tree.filter(javalang.tree.ClassDeclaration):\n",
    "        if(n.name == class_name):\n",
    "            df = generate_feature_dataframe(n, get_methods(n), get_fields(n))\n",
    "            df = df.fillna(0)\n",
    "            column_names = df.columns.difference(['method_name'])\n",
    "            df[column_names] = df[column_names].astype(int)\n",
    "            if not save_directory_path.endswith('/') != './':\n",
    "                save_directory_path = save_directory_path+'/'\n",
    "            df.to_csv(save_directory_path+class_name+'.csv', index=False)\n",
    "            class_features[class_name] = df\n",
    "    return class_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_features = {}\n",
    "for path in god_df['path']:\n",
    "    class_features.update(extract_feature_vectors(path, save_directory_path='./generated/feature_vectors/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class CoreDocumentImpl has 117 vectors and 139 features\n",
      "Class DTDGrammar has 91 vectors and 166 features\n",
      "Class XSDHandler has 106 vectors and 226 features\n",
      "Class XIncludeHandler has 108 vectors and 200 features\n"
     ]
    }
   ],
   "source": [
    "for k,v in class_features.items():\n",
    "    num_vectors = v.shape[0]\n",
    "    num_features = v.shape[1] - 1\n",
    "    print(f'Class {k} has {num_vectors} vectors and {num_features} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./generated/feature_vectors/CoreDocumentImpl.csv',\n",
       " './generated/feature_vectors/DTDGrammar.csv',\n",
       " './generated/feature_vectors/XIncludeHandler.csv',\n",
       " './generated/feature_vectors/XSDHandler.csv']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_csv_files_paths = ['./generated/feature_vectors/' + str(f) for f in os.listdir('./generated/feature_vectors') if f != 'god_classes.csv']\n",
    "feature_csv_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './generated/clusterings/'\n",
    "def k_means_clustering(n_clusters: int, path_to_featurevec_csv: str = None, save: bool = False, get:bool = False) -> None:\n",
    "    df = pd.read_csv(path_to_featurevec_csv)\n",
    "\n",
    "    X = df.drop('method_name', axis=1).values\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit_predict(X)\n",
    "\n",
    "    df_for_csv = df['method_name'].to_frame()\n",
    "    df_for_csv['cluster_id'] = kmeans  \n",
    "    cols = df_for_csv.columns.tolist()\n",
    "    df_for_csv = df_for_csv[cols[-1:] + cols[:-1]]\n",
    "    \n",
    "    class_name = path_to_featurevec_csv.split('/')[-1].split('.')[0]\n",
    "    if save:\n",
    "        df_for_csv.sort_values(by='cluster_id').to_csv(save_path+class_name + '_kmeans_'+str(n_clusters)+'.csv', index=False)\n",
    "    if get:\n",
    "        return df_for_csv\n",
    "    \n",
    "def agglomerative_clustering(path_to_featurevec_csv: str, n_clusters: int, save: bool = False, get: bool = False, linkage='complete') -> None:\n",
    "    df = pd.read_csv(path_to_featurevec_csv)\n",
    "    X = df.drop('method_name', axis=1).values\n",
    "    kmeans = AgglomerativeClustering(n_clusters=n_clusters,linkage=linkage).fit_predict(X)\n",
    "\n",
    "    df_for_csv = df['method_name'].to_frame()\n",
    "    df_for_csv['cluster_id'] = kmeans  \n",
    "    cols = df_for_csv.columns.tolist()\n",
    "    df_for_csv = df_for_csv[cols[-1:] + cols[:-1]]\n",
    "    \n",
    "    class_name = path_to_featurevec_csv.split('/')[-1].split('.')[0]\n",
    "    if save:\n",
    "        df_for_csv.sort_values(by='cluster_id').to_csv(save_path+class_name + '_agglomerative_'+str(linkage)+'_'+str(n_clusters)+'.csv', index=False)\n",
    "    if get:\n",
    "        return df_for_csv\n",
    "    \n",
    "\n",
    "def silhouette(path_to_featurevec_csv: str, clustering_csv_path: str = None, max_clusters: int = None, min_clusters: int = 2):\n",
    "    df_feature_vector = pd.read_csv(path_to_featurevec_csv).drop('method_name', axis=1).values\n",
    "    \n",
    "    if clustering_csv_path: \n",
    "        df_clustering = pd.read_csv(clustering_csv_path).drop('method_name', axis=1).values.ravel()\n",
    "        return silhouette_score(df_feature_vector, df_clustering)\n",
    "    else:\n",
    "        kmean_dict = {}\n",
    "        agglom_dict_complete = {}\n",
    "        agglom_dict_single = {}\n",
    "        for k in range(min_clusters, max_clusters + 1):\n",
    "            df_clustering = k_means_clustering(n_clusters=k, path_to_featurevec_csv=path_to_featurevec_csv, get=True,save=True).drop('method_name', axis=1).values.ravel()\n",
    "            kmean_dict[k] = silhouette_score(df_feature_vector, df_clustering)\n",
    "\n",
    "            df_clustering = agglomerative_clustering(path_to_featurevec_csv=path_to_featurevec_csv, n_clusters=k, get = True,save=True).drop('method_name', axis=1).values.ravel()\n",
    "            agglom_dict_complete[k] = silhouette_score(df_feature_vector, df_clustering)\n",
    "\n",
    "            df_clustering = agglomerative_clustering(path_to_featurevec_csv=path_to_featurevec_csv, n_clusters=k, get = True,save=True,linkage='single').drop('method_name', axis=1).values.ravel()\n",
    "            agglom_dict_single[k] = silhouette_score(df_feature_vector, df_clustering)\n",
    "\n",
    "        return kmean_dict, agglom_dict_complete, agglom_dict_single\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (46). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (47). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (48). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (49). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (50). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (51). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (52). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (53). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (54). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (55). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (56). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (57). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (58). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (59). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\fabia\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474: ConvergenceWarning: Number of distinct clusters (45) found smaller than n_clusters (60). Possibly due to duplicate points in X.\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "for p in feature_csv_files_paths:\n",
    "    class_name = p.split('/')[-1].split('.')[0]\n",
    "    scores[class_name] = silhouette(p, max_clusters=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './generated/silhouette_scores/'\n",
    "for n,v in scores.items():\n",
    "    kmeans, agglo_complete, agglo_single = v\n",
    "    df = pd.DataFrame(columns=['k', 'kmeans', 'agglomerative_complete', 'agglomerative_single'])\n",
    "    for k in kmeans.keys():\n",
    "        df.loc[len(df)] = [k, kmeans[k], agglo_complete[k], agglo_single[k]]\n",
    "    df.to_csv(save_path+'csv/'+n+'_silhouette_scores.csv', index=False)\n",
    "    \n",
    "    df.plot(x='k', y=['kmeans', 'agglomerative_complete', 'agglomerative_single'])\n",
    "    plt.title(n+' silhouette scores')\n",
    "\n",
    "    max_score = max(max(kmeans.values()), max(agglo_complete.values()), max(agglo_single.values()))\n",
    "    methods = []\n",
    "    if max_score in kmeans.values():\n",
    "        methods.append('kmeans')\n",
    "    if max_score in agglo_complete.values():\n",
    "        methods.append('agglomerative_complete')\n",
    "    if max_score in agglo_single.values():\n",
    "         methods.append('agglomerative_single')\n",
    "    \n",
    "    k = [[k for k, v in kmeans.items() if v == max_score]]\n",
    "    k.append([k for k, v in agglo_complete.items() if v == max_score])\n",
    "    k.append([k for k, v in agglo_single.items() if v == max_score])\n",
    "    plt.yticks(list(plt.yticks()[0]) + [max_score])\n",
    "    plt.yticks(fontsize=7)\n",
    "    plt.axhline(y=max_score, color='r', linestyle='--', label=f'k = {k}, method = {methods}')\n",
    "    plt.legend(loc = 'lower center', bbox_to_anchor = (0.5,-0.4))\n",
    "    plt.savefig(save_path+'plots/'+n+'_silhouette_scores.png', bbox_inches='tight')\n",
    "\n",
    "        \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ground_truth(path_featurevec_csv: str, path_keywords_list:str, save_path = './') -> None:\n",
    "\n",
    "    with open(path_keywords_list, 'r') as f:\n",
    "        keywords_list = f.read().splitlines()\n",
    "    df = pd.read_csv(path_featurevec_csv)\n",
    "\n",
    "    ground_truths = {}\n",
    "    for method in df['method_name'].values:\n",
    "        for keyword in keywords_list:\n",
    "            if keyword in method.lower():\n",
    "                if keyword not in ground_truths:\n",
    "                    ground_truths[keyword] = []\n",
    "                ground_truths[keyword].append(method)\n",
    "                break\n",
    "        else:\n",
    "            if 'none' not in ground_truths:\n",
    "                ground_truths['none'] = []\n",
    "            ground_truths['none'].append(method)\n",
    "\n",
    "    df_to_csv = df['method_name'].to_frame()\n",
    "    df_to_csv['cluster_id'] = -1\n",
    "    cols = df_to_csv.columns.tolist()\n",
    "    df_to_csv = df_to_csv[cols[-1:] + cols[:-1]]\n",
    "    for i, keyword in enumerate(ground_truths):\n",
    "        for method in ground_truths[keyword]:\n",
    "            df_to_csv.loc[df_to_csv['method_name'] == method, 'cluster_id'] = i\n",
    "\n",
    "    df_to_csv.sort_values(by='cluster_id', inplace=True)\n",
    "    file_name = path_featurevec_csv.split('/')[-1].split('.')[0]\n",
    "    file_name = save_path+'ground_truth_'+file_name+'.csv'\n",
    "    df_to_csv.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in feature_csv_files_paths:\n",
    "    get_ground_truth(p, './keywords_list.txt', save_path='./generated/ground_truth/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_CoreDocumentImpl.csv\n",
      "cluster_id\n",
      "0     69\n",
      "1     14\n",
      "2      6\n",
      "6      6\n",
      "3      5\n",
      "4      4\n",
      "8      4\n",
      "9      3\n",
      "5      2\n",
      "7      2\n",
      "10     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "ground_truth_DTDGrammar.csv\n",
      "cluster_id\n",
      "0    64\n",
      "2    17\n",
      "3     6\n",
      "1     2\n",
      "4     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "ground_truth_XIncludeHandler.csv\n",
      "cluster_id\n",
      "0    90\n",
      "5     9\n",
      "3     4\n",
      "1     2\n",
      "4     2\n",
      "2     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "ground_truth_XSDHandler.csv\n",
      "cluster_id\n",
      "1    57\n",
      "3    24\n",
      "0    18\n",
      "4     3\n",
      "5     3\n",
      "2     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for f in os.listdir('./generated/ground_truth/'):\n",
    "    df = pd.read_csv('./generated/ground_truth/'+f)\n",
    "    print(f)\n",
    "    print(df['cluster_id'].value_counts())\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intrapairs(df: pd.DataFrame) -> list:\n",
    "    intrapairs = []\n",
    "    for _, group in df.groupby('cluster_id'):\n",
    "        if len(group) > 1:\n",
    "            for i in range(len(group)):\n",
    "                for j in range(i+1, len(group)):\n",
    "                    temp_set = set()\n",
    "                    temp_set.update([group.iloc[i]['method_name'], group.iloc[j]['method_name']])\n",
    "                    if temp_set not in intrapairs:\n",
    "                        intrapairs.append(temp_set)\n",
    "    return intrapairs\n",
    "\n",
    "def get_intersections(intra1:list, intra2:list ) -> list:\n",
    "    intersections = []\n",
    "    for p1 in intra1:\n",
    "        if p1 in intra2:\n",
    "            intersections.append(p1)\n",
    "    return intersections\n",
    "\n",
    "def get_precision_recall(path_cluster_csv: str, path_ground_truth: str) -> tuple:\n",
    "    df_d = pd.read_csv(path_cluster_csv)\n",
    "    df_g = pd.read_csv(path_ground_truth)\n",
    "\n",
    "    intra_d = get_intrapairs(df_d)\n",
    "    intra_g = get_intrapairs(df_g)\n",
    "    inter = get_intersections(intra_d, intra_g)\n",
    "\n",
    "    p = len(inter) / len(intra_d)\n",
    "    r = len(inter) / len(intra_g)\n",
    "\n",
    "    return p,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_dict = {}\n",
    "for path_ground in os.listdir('./generated/ground_truth'):\n",
    "    class_name = path_ground.split('_')[2].split('.')[0]\n",
    "    pr_dict[class_name] = {\n",
    "        'agglomerative_complete': {},\n",
    "        'agglomerative_single': {},\n",
    "        'kmeans': {}\n",
    "    }\n",
    "    for algo in pr_dict[class_name].keys():\n",
    "        file_name = class_name+'_'+algo\n",
    "        for path_cluster in os.listdir('./generated/clusterings/'):\n",
    "            if file_name in path_cluster:\n",
    "                k = int(path_cluster.split('_')[-1].split('.')[0])\n",
    "                p,r = get_precision_recall('./generated/clusterings/'+path_cluster, './generated/ground_truth/'+path_ground)\n",
    "                pr_dict[class_name][algo][k] = (p,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./generated/pr_dict.pickle', 'wb') as handle:\n",
    "    pickle.dump(pr_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each class in pr_dict, plot precision recall curve for each algorithm\n",
    "for class_name in pr_dict.keys():\n",
    "    # create a figure with 2 plots\n",
    "    # create more distance between the plots\n",
    "    fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "    fig.suptitle(class_name)\n",
    "    for algo in pr_dict[class_name].keys():\n",
    "        p = []\n",
    "        r = []\n",
    "        ks = []\n",
    "        for k in sorted(pr_dict[class_name][algo].keys()):\n",
    "            p.append(pr_dict[class_name][algo][k][0])\n",
    "            r.append(pr_dict[class_name][algo][k][1])\n",
    "            ks.append(int(k))\n",
    "        axs[0].plot(ks, p, label=algo)\n",
    "        axs[1].plot(ks, r, label=algo)\n",
    "    optimal = 45 if class_name == 'CoreDocumentImpl' else 2\n",
    "    p_optimal_y_tick = pr_dict[class_name]['agglomerative_single'][optimal][0]\n",
    "    r_optimal_y_tick = pr_dict[class_name]['agglomerative_single'][optimal][1]\n",
    "    axs[0].set_yticks(list(axs[0].get_yticks()) + [p_optimal_y_tick])\n",
    "    axs[1].set_yticks(list(axs[1].get_yticks()) + [r_optimal_y_tick])\n",
    "    axs[0].axvline(x=optimal, color='red', linestyle='--', label='agglomerative single\\noptimal k='+str(optimal)+', p='+str(round(p_optimal_y_tick, 2)))\n",
    "    axs[1].axvline(x=optimal, color='red', linestyle='--', label='agglomerative single\\noptimal k='+str(optimal)+', p='+str(round(r_optimal_y_tick, 2)))\n",
    "    axs[0].set_title('Precision')\n",
    "    axs[1].set_title('Recall')\n",
    "    axs[0].set_xlabel('k')\n",
    "    axs[1].set_xlabel('k')\n",
    "    axs[0].set_ylabel('Precision')\n",
    "    axs[1].set_ylabel('Recall')\n",
    "    axs[0].grid()\n",
    "    axs[1].grid()\n",
    "    axs[0].legend()\n",
    "    axs[1].legend()\n",
    "    plt.savefig('./generated/pr_curves/'+class_name+'_pr_curve.png')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
