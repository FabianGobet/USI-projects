{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ-a3cM1EBhC"
      },
      "outputs": [],
      "source": [
        "# Packages\n",
        "############################\n",
        "'''\n",
        "Template for the 4th assignment\n",
        "Student: Fabian Gobet\n",
        "'''\n",
        "import os\n",
        "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from accelerate import Accelerator\n",
        "import random\n",
        "import re\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "import statistics\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "import itertools\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPVvoUpsEBhD"
      },
      "outputs": [],
      "source": [
        "# Classes\n",
        "############################\n",
        "# Vocabulary class\n",
        "class Vocabulary:\n",
        "    '''\n",
        "    Class for dealing with our corpus\n",
        "    '''\n",
        "\n",
        "    def __init__(self, name, sentences):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            name (str): name of the language\n",
        "            pairs (list): list of pairs of sentences\n",
        "        \"\"\"\n",
        "        self.name = name\n",
        "        self.word2index = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}\n",
        "        self.index2word = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\"}\n",
        "        for s in sentences:\n",
        "            self.add_sentence(s)\n",
        "\n",
        "\n",
        "    def add_word(self, word):\n",
        "        '''\n",
        "        Add a word to the vocabulary\n",
        "        :param word: a string\n",
        "        '''\n",
        "        # TODO: add the word to the vocabulary\n",
        "        if not word in self.word2index:\n",
        "            self.word2index[word] = len(self.word2index)\n",
        "            self.index2word[len(self.index2word)] = word\n",
        "\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        '''\n",
        "        Add a sentence to the vocabulary\n",
        "        :param sentence: list of strings (words)\n",
        "        '''\n",
        "        # TODO add the sentence to the vocabulary, this method will call the add_word method\n",
        "        for word in sentence:\n",
        "            self.add_word(word)\n",
        "\n",
        "\n",
        "# Dataset class\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, vocabulary, pairs_refs, sentences):\n",
        "        # TODO We want vocabulary and pairs to be attributes of the class\n",
        "        self.vocabulary = vocabulary\n",
        "        self.pairs = pairs_refs\n",
        "        self.sentences = sentences\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO how many pairs do we have?\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        # TODO returns two tensors (question, answer) of the pair at index ix\n",
        "        # TODO the tensors should be of type torch.tensor and should contain integers (word indices)\n",
        "        q,a = self.pairs[ix]\n",
        "        q = torch.tensor([self.vocabulary.word2index[word] for word in self.sentences[q]])\n",
        "        a = torch.tensor([self.vocabulary.word2index[word] for word in [\"<SOS>\"]+self.sentences[a]])\n",
        "        return q,a\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    '''\n",
        "    Adapted from\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    '''\n",
        "    def __init__(self, d_model, dropout=0.0, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
        "                             * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        try:\n",
        "            assert x.size(0) < self.max_len\n",
        "        except:\n",
        "            print(\"The length of the sequence is bigger than the max_len of the positional encoding. Increase the max_len or provide a shorter sequence.\")\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=512, pad_id=0, encoder_layers=6, decoder_layers=6, dim_feedforward=2048, num_heads=8, dropout_transformer=0.1, dropout_posenconding=0):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO add an embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "        # TODO add a positional encoding layer\n",
        "        self.pos_encoder = PositionalEncoding(d_model,dropout=dropout_posenconding)\n",
        "\n",
        "        # TODO add a transformer layer, you can use nn.Transformer. You can use the default values for the parameters, but what about batch_first?\n",
        "        self.transformer = nn.Transformer(d_model=d_model, nhead=num_heads, num_encoder_layers=encoder_layers, num_decoder_layers=decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout_transformer, batch_first=True)\n",
        "\n",
        "        # TODO add a linear layer. Note: output should be probability distribution over the vocabulary\n",
        "        self.linear = nn.Linear(d_model, vocab_size, bias=False)\n",
        "\n",
        "        # Stuff you may need\n",
        "        self.vocab_size = vocab_size\n",
        "        self.pad_id = pad_id\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "    def create_padding_mask(self, x, pad_id=0):\n",
        "        # TODO create a boolean mask for the <PAD> tokens\n",
        "        return x.eq(pad_id)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        # S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number\n",
        "        # src: (N, S)\n",
        "        # tgt: (N, T)\n",
        "        # src_pad_mask: (N, S)\n",
        "        # tgt_pad_mask: (N, T)\n",
        "        # mask the future : (N * num_heads, T, T)\n",
        "\n",
        "        src_pad_mask = self.create_padding_mask(src, self.pad_id) # (N, S)\n",
        "        tgt_pad_mask = self.create_padding_mask(tgt, self.pad_id) # (N, T)\n",
        "\n",
        "        src = self.embedding(src)\n",
        "        tgt = self.embedding(tgt)\n",
        "\n",
        "        src = self.pos_encoder(src)  # (N, S, E)\n",
        "        tgt = self.pos_encoder(tgt) # (N, T, E)\n",
        "\n",
        "        # Mask the memory\n",
        "        memory_key_padding_mask = src_pad_mask  # (N, S)\n",
        "\n",
        "        # Mask the future\n",
        "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt.size(1), dtype=torch.bool).to(tgt.device) # (T, T)\n",
        "\n",
        "        # Expand to make it N * num_heads, T, T\n",
        "        tgt_mask = tgt_mask.unsqueeze(0).repeat(tgt.size(0) * self.num_heads, 1, 1) # (N, T, T)\n",
        "\n",
        "        # Transformer\n",
        "        output = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask=src_pad_mask,tgt_key_padding_mask=tgt_pad_mask, memory_key_padding_mask=memory_key_padding_mask) # (N, T, E)\n",
        "        # Linear layer\n",
        "        output = self.linear(output) # (N, T, V)\n",
        "        return output\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = float('inf')\n",
        "        self.early_stop = False\n",
        "        self.best_model = None\n",
        "\n",
        "    def check_early_stop(self, current_loss, model):\n",
        "        if current_loss < self.best_loss:\n",
        "            self.best_loss = current_loss\n",
        "            self.counter = 0\n",
        "            self.best_model = model.state_dict()\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        return self.early_stop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Methods new\n",
        "############################\n",
        "def clear_punctuation(s):\n",
        "    '''\n",
        "    This function removes all the punctuation from a sentence and insert a blank between any letter and !?.\n",
        "    :param s: a string\n",
        "    :return: the \"cleaned\" string\n",
        "    '''\n",
        "    # Remove all the character that are not letters, puntuation or numbers\n",
        "    s = re.sub(r\"[^a-zA-Z.!?,']+\", r\" \", s)\n",
        "    # Insert a blank between any letter and !?. using regex\n",
        "    #s = re.sub(r\"([a-zA-Z])([!?.])\", r\"\\1 \\2\", s)\n",
        "    return s\n",
        "\n",
        "\n",
        "def pickle_dump(obj, PATH, name):\n",
        "    '''\n",
        "    Save an object to a pickle file\n",
        "    :param obj: object to save\n",
        "    :param path: path to the pickle file\n",
        "    '''\n",
        "    if not os.path.exists(PATH):\n",
        "        os.makedirs(PATH)\n",
        "\n",
        "    with open(PATH+name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "def pickle_load(PATH):\n",
        "    '''\n",
        "    Load an object from a pickle file\n",
        "    :param path: path to the pickle file\n",
        "    :return: the loaded object\n",
        "    '''\n",
        "    with open(PATH, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def print_random_elements(collection, k=5):\n",
        "    '''\n",
        "    Print k random elements from a collection\n",
        "    :param collection: list of elements\n",
        "    :param k: number of elements to print\n",
        "    '''\n",
        "    random_elements = random.sample(collection, k=k)\n",
        "    for e in random_elements:\n",
        "        print(e)\n",
        "\n",
        "def collate_fn(batch,pad_idx):\n",
        "  data, targets = zip(*batch)\n",
        "  padded_data = nn.utils.rnn.pad_sequence(data, batch_first=True, padding_value=pad_idx)\n",
        "  padded_targets = nn.utils.rnn.pad_sequence(targets, batch_first=True, padding_value=pad_idx)\n",
        "  return padded_data, padded_targets\n",
        "\n",
        "def checkpoint(model,optimizer,path='./MyFiles/',save_name=None):\n",
        "  dic = {\n",
        "      'model_state' : model.state_dict(),\n",
        "      'optimizer' : optimizer.state_dict()\n",
        "  }\n",
        "  if save_name is not None:\n",
        "    torch.save(dic, path+save_name+\".pt\")\n",
        "  return dic\n",
        "\n",
        "def load_checkpoint(path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model_state = checkpoint['model_state']\n",
        "    optimizer_state = checkpoint['optimizer']\n",
        "    return model_state,optimizer_state\n",
        "\n",
        "def check_content_txt_files(convo_path, lines_path, num_elements):\n",
        "    '''\n",
        "    Check the content of the files\n",
        "    :param convo_path: path to the movie_conversations.txt file\n",
        "    :param lines_path: path to the movie_lines.txt file\n",
        "    :param num_elements: number of elements to print\n",
        "    '''\n",
        "    # Inspect movie_conversations.txt\n",
        "    with open(convo_path, 'r') as conv_file:\n",
        "        for i in range(num_elements):\n",
        "            line = conv_file.readline()\n",
        "\n",
        "    # Inspect movie_lines.txt\n",
        "    with open(lines_path, 'r') as lines_file:\n",
        "        for i in range(num_elements):\n",
        "            line = lines_file.readline()\n",
        "\n",
        "def get_reference_pairs(convo_path):\n",
        "    '''\n",
        "    Get the reference pairs\n",
        "    :return: a list of pairs of references of sentences\n",
        "    '''\n",
        "    ref_pairs = []\n",
        "    with open(convo_path, 'r') as conv_file:\n",
        "        for line in conv_file:\n",
        "            conversation = line.strip().split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").split(\",\")\n",
        "            for i in range(len(conversation) - 1):\n",
        "                ref_pairs.append((conversation[i].strip(), conversation[i+1].strip()))\n",
        "    return ref_pairs\n",
        "    \n",
        "\n",
        "def normalize_sentences(path_lines):\n",
        "    lines_dict = {}\n",
        "    empty_lines = 0\n",
        "    with open(path_lines, 'r', encoding='cp1252') as lines_file:\n",
        "        for full_line in lines_file:\n",
        "            line_split = full_line.split(' +++$+++ ')\n",
        "            line = line_split[-1]\n",
        "            line, is_line_empty = process_sentence(line)\n",
        "            if is_line_empty == 1:\n",
        "                empty_lines += 1\n",
        "            else:\n",
        "                lines_dict.update({line_split[0] : line})\n",
        "    return lines_dict, empty_lines\n",
        "\n",
        "def process_sentence(line):\n",
        "    is_line = 0\n",
        "    line = line.replace('\\n','').replace(\"<u>\",\"\").replace(\"</u>\",\"\").lower()\n",
        "    line = re.sub('-+','',line)\n",
        "    line = re.sub(r\"[^a-zA-Z.!?,']+\", r\" \", line)\n",
        "    line = re.sub(r\"i'm\", \"i am\", line, flags=re.I)\n",
        "    line = re.sub(r\"it's\", \"it is\", line, flags=re.I)\n",
        "    line = re.sub(r\"he's\", \"he is\", line, flags=re.I)\n",
        "    line = re.sub(r\"she's\", \"she is\", line, flags=re.I)\n",
        "    line = re.sub(r\"can't\", \"can not\", line, flags=re.I)\n",
        "    line = re.sub(r\"that's\", \"that is\", line, flags=re.I)\n",
        "    line = re.sub(r\"there's\", \"there is\", line, flags=re.I)\n",
        "    line = re.sub(r\"what's\", \"what is\", line, flags=re.I)\n",
        "    line = re.sub(r\"where's\", \"where is\", line, flags=re.I)\n",
        "    line = re.sub(r\"how's\", \"how is\", line, flags=re.I)\n",
        "    line = re.sub(r'\\.+(?:\\s*\\.{1,})+', '...', line, flags=re.I)\n",
        "    line = re.sub(r'!+(?:\\s*!{1,})+', '!', line, flags=re.I)\n",
        "    line = re.sub(r\"\\'ll\", \" will\", line, flags=re.I)\n",
        "    line = re.sub(r\"\\'em\", \" them\", line, flags=re.I)\n",
        "    line = re.sub(r\"\\'s\", \" is\", line, flags=re.I)\n",
        "    line = re.sub(r\"\\'ve\", \" have\", line, flags=re.I)\n",
        "    line = re.sub(r\"\\'re\", \" are\", line, flags=re.I)\n",
        "    line = re.sub(r\"\\'d\", \" would\", line, flags=re.I)\n",
        "    line = re.sub(r\"n't\", \" not\", line, flags=re.I)\n",
        "    line = re.sub(r\"won't\", \"will not\", line, flags=re.I)\n",
        "    line = re.sub(r\"can't\", \"cannot\", line, flags=re.I)\n",
        "    line = line.strip()\n",
        "    if line == '' or line.isspace():\n",
        "        is_line += 1\n",
        "    else:\n",
        "        line = TreebankWordTokenizer().tokenize(line)\n",
        "        if line[-1] not in ['.','!','?','...']:\n",
        "            line.append('.')\n",
        "    return line, is_line\n",
        "\n",
        "def get_valid_refs(ref_pairs, lines):\n",
        "    valid_refs = set()\n",
        "    for p in ref_pairs:\n",
        "        if p[0] in lines and p[1] in lines:\n",
        "            valid_refs.add(p[0])\n",
        "            valid_refs.add(p[1])\n",
        "    return valid_refs\n",
        "\n",
        "def generate_primitive_valid_pairs(all_ref_pairs, lines):\n",
        "    chosen_sentences = {}\n",
        "    chosen_ref_pairs = []\n",
        "\n",
        "    for p in all_ref_pairs:\n",
        "        if p[0] in lines and p[1] in lines:\n",
        "            chosen_sentences.update({p[0]: lines[p[0]]+['<EOS>'], p[1]: lines[p[1]]+['<EOS>']})\n",
        "            chosen_ref_pairs.append(p)\n",
        "    return chosen_sentences, chosen_ref_pairs\n",
        "\n",
        "def eliminate_long_sentences(chosen_sentences, chosen_ref_pairs, max_length):\n",
        "    rule_out_sentences_refs = set()\n",
        "    chosen_sentences2 = {}\n",
        "    chosen_ref_pairs2 = []\n",
        "\n",
        "    for k,v in chosen_sentences.items():\n",
        "        if len(v) > max_length:\n",
        "            rule_out_sentences_refs.add(k)\n",
        "\n",
        "    for p in chosen_ref_pairs:\n",
        "        if p[0] not in rule_out_sentences_refs and p[1] not in rule_out_sentences_refs:\n",
        "            chosen_sentences2.update({p[0]: chosen_sentences[p[0]]})\n",
        "            chosen_sentences2.update({p[1]: chosen_sentences[p[1]]})\n",
        "            chosen_ref_pairs2.append(p)\n",
        "\n",
        "    return chosen_sentences2,chosen_ref_pairs2,rule_out_sentences_refs\n",
        "\n",
        "def eliminate_sentences_with_rare_words(chosen_sentences, chosen_ref_pairs, rule_out_words):\n",
        "    rule_out_sentences_refs = set()\n",
        "    chosen_sentences2 = {}\n",
        "    chosen_ref_pairs2 = []\n",
        "\n",
        "    for k,v in chosen_sentences.items():\n",
        "        if any(word in rule_out_words for word in v):\n",
        "            rule_out_sentences_refs.add(k)\n",
        "\n",
        "    for p in chosen_ref_pairs:\n",
        "        if p[0] not in rule_out_sentences_refs and p[1] not in rule_out_sentences_refs:\n",
        "            chosen_sentences2.update({p[0]: chosen_sentences[p[0]]})\n",
        "            chosen_sentences2.update({p[1]: chosen_sentences[p[1]]})\n",
        "            chosen_ref_pairs2.append(p)\n",
        "\n",
        "    return chosen_sentences2,chosen_ref_pairs2,rule_out_sentences_refs\n",
        "\n",
        "def count_words(chosen_ref_pairs, chosen_sentences):\n",
        "    word_counts = {}\n",
        "    for p in chosen_ref_pairs:\n",
        "        for r in p:\n",
        "            for w in chosen_sentences[r]:\n",
        "                if w in word_counts:\n",
        "                    word_counts[w] += 1\n",
        "                else:\n",
        "                    word_counts[w] = 1\n",
        "    num_words = sum(word_counts.values())\n",
        "    return word_counts, num_words\n",
        "\n",
        "\n",
        "def extract_sentences_from_refs(chosen_ref_pairs, chosen_sentences):\n",
        "    chosen_sentences2 = {}\n",
        "    for p in chosen_ref_pairs:\n",
        "        chosen_sentences2.update({p[0]: chosen_sentences[p[0]]})\n",
        "        chosen_sentences2.update({p[1]: chosen_sentences[p[1]]})\n",
        "    return chosen_sentences2 \n",
        "\n",
        "\n",
        "\n",
        "def prime_factors(n):\n",
        "    i = 2\n",
        "    while i * i <= n:\n",
        "        if n % i == 0:\n",
        "            n /= i\n",
        "            yield i\n",
        "        else:\n",
        "            i += 1\n",
        "    if n > 1:\n",
        "        yield n\n",
        "\n",
        "\n",
        "def prod(iterable):\n",
        "    result = 1\n",
        "    for i in iterable:\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_divisors(n):\n",
        "    pf = prime_factors(n)\n",
        "\n",
        "    pf_with_multiplicity = collections.Counter(pf)\n",
        "\n",
        "    powers = [\n",
        "        [factor ** i for i in range(count + 1)]\n",
        "        for factor, count in pf_with_multiplicity.items()\n",
        "    ]\n",
        "\n",
        "    for prime_power_combo in itertools.product(*powers):\n",
        "        yield prod(prime_power_combo)\n",
        "\n",
        "\n",
        "def create_pairs(path='./MyFiles/',savename=\"result\",max_length=26,word_frequency_discard=10,verbose=True):\n",
        "\n",
        "    if verbose:\n",
        "        check_content_txt_files('./Data/movie_conversations.txt', './Data/movie_lines.txt', 5)\n",
        "\n",
        "    all_ref_pairs = get_reference_pairs('./Data/movie_conversations.txt')\n",
        "\n",
        "    if verbose:\n",
        "        print_random_elements(all_ref_pairs)\n",
        "\n",
        "    lines,empty_lines = normalize_sentences('./Data/movie_lines.txt')\n",
        "\n",
        "    if verbose:\n",
        "        print('Number of empty lines: {}'.format(empty_lines))\n",
        "        print_random_elements(list(lines.values()))\n",
        "\n",
        "    chosen_sentences, chosen_ref_pairs = generate_primitive_valid_pairs(all_ref_pairs, lines)\n",
        "\n",
        "    if verbose:\n",
        "        print_random_elements(chosen_ref_pairs)\n",
        "\n",
        "        # Filter out the sentences that are too long\n",
        "        # Compute the length of each sentence\n",
        "        # Compute the mean and standard deviation for sentence lengths\n",
        "        sentence_lengths = []\n",
        "        for p in chosen_ref_pairs:\n",
        "            sentence_lengths.append(len(chosen_sentences[p[0]]))\n",
        "            sentence_lengths.append(len(chosen_sentences[p[1]]))\n",
        "\n",
        "        mean_length = statistics.mean(sentence_lengths)\n",
        "        std_dev = statistics.stdev(sentence_lengths)\n",
        "\n",
        "        print('Mean sentence length: {}'.format(mean_length))\n",
        "        print('Standard deviation: {}'.format(std_dev))\n",
        "        print('Max sentence length: {}'.format(max(sentence_lengths)))\n",
        "        print('Min sentence length: {}'.format(min(sentence_lengths)))\n",
        "\n",
        "        # Plot the histogram with logarithmic scale on the y-axis\n",
        "        plt.hist(sentence_lengths, density=True, bins=40)\n",
        "        plt.xlabel('Sentence Length')\n",
        "        plt.ylabel('Frequency (log scale)')\n",
        "        plt.title('Sentence Length Distribution')\n",
        "        plt.yscale('log')  # Set y-axis scale to logarithmic\n",
        "        plt.axvline(x=mean_length, color='r', linestyle='--', label='Mean')  # Add vertical line for the mean\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Compute the frequency of each sentence length\n",
        "        length_counts = {}\n",
        "        for length in sentence_lengths:\n",
        "            if length in length_counts:\n",
        "                length_counts[length] += 1\n",
        "            else:\n",
        "                length_counts[length] = 1\n",
        "\n",
        "        # Sort the sentence lengths in ascending order\n",
        "        sorted_lengths = sorted(length_counts.keys())\n",
        "\n",
        "        # Compute the accumulated frequency percentage\n",
        "        total_sentences = len(sentence_lengths)\n",
        "        accumulated_percentage = 0\n",
        "        percentage_values = []\n",
        "        for length in sorted_lengths:\n",
        "            frequency = length_counts[length]\n",
        "            percentage = (frequency / total_sentences) * 100\n",
        "            accumulated_percentage += percentage\n",
        "            percentage_values.append(accumulated_percentage)\n",
        "\n",
        "        # Plot the sentence lengths and their accumulated frequency percentage\n",
        "        plt.plot(sorted_lengths, percentage_values)\n",
        "        plt.xlabel('Sentence Length')\n",
        "        plt.ylabel('Accumulated Frequency Percentage')\n",
        "        plt.title('Sentence Length Distribution')\n",
        "        plt.show()\n",
        "\n",
        "    initial_num_sentences = len(chosen_sentences)\n",
        "    initial_num_pairs = len(chosen_ref_pairs)\n",
        "    chosen_sentences2,chosen_ref_pairs2,rule_out_sentences_refs = eliminate_long_sentences(chosen_sentences, chosen_ref_pairs, max_length) \n",
        "\n",
        "    if verbose:\n",
        "        print('Initial number of sentences: {}'.format(initial_num_sentences))\n",
        "        print('Current number of senteces: {}'.format(initial_num_sentences-len(rule_out_sentences_refs)))\n",
        "        print('Initial number of pairs: {}'.format(initial_num_pairs))\n",
        "        print('Current number of pairs: {}'.format(initial_num_pairs-len(chosen_ref_pairs2)))\n",
        "\n",
        "\n",
        "    word_counts, num_words = count_words(chosen_ref_pairs2, chosen_sentences2)\n",
        "\n",
        "    if verbose:\n",
        "        print('Number of words: {}'.format(num_words))\n",
        "        print('Number of unique words: {}'.format(len(word_counts)))\n",
        "\n",
        "        plt.plot(range(len(word_counts)), list(word_counts.values()))\n",
        "        plt.xlabel('Word index')\n",
        "        plt.ylabel('Word frequency')\n",
        "        plt.suptitle('Word frequency distribution')\n",
        "        plt.title('Frequency per word')\n",
        "        plt.show()\n",
        "\n",
        "        # Compute the mean and standard deviation for word counts\n",
        "        mean_value = statistics.mean(word_counts.values())\n",
        "        print(\"Mean value:\", mean_value)\n",
        "        print(\"Max value:\", max(word_counts.values()))\n",
        "        print(\"Min value:\", min(word_counts.values()))\n",
        "\n",
        "        # Plot the distribution of word counts < mean\n",
        "        dict_lower_mean = {k: v for k, v in word_counts.items() if v < mean_value}\n",
        "        sorted_dict_lower_mean = dict(sorted(dict_lower_mean.items(), key=lambda x: x[1], reverse=True))\n",
        "\n",
        "        plt.plot(range(len(sorted_dict_lower_mean)), sorted_dict_lower_mean.values())\n",
        "        plt.xlabel('Word index')\n",
        "        plt.ylabel('Word count')\n",
        "        plt.suptitle('Word count distribution')\n",
        "        plt.title('Words with count < mean')\n",
        "        plt.show()\n",
        "\n",
        "    # Frequency threshold and filter out the words that are too rare\n",
        "    rule_out_words = [k for k, v in word_counts.items() if v < word_frequency_discard]\n",
        "    chosen_sentences3,chosen_ref_pairs3,rule_out_sentences_refs = eliminate_sentences_with_rare_words(chosen_sentences2, chosen_ref_pairs2, rule_out_words)\n",
        "\n",
        "    if verbose:\n",
        "        print('Current number of senteces: {} ({:.2f}% of total)'.format(initial_num_sentences-len(chosen_sentences3),(initial_num_sentences-len(chosen_sentences3))/initial_num_sentences*100))\n",
        "        print('Current number of pairs: {} ({:.2f}% of total)'.format(initial_num_pairs-len(chosen_ref_pairs3),(initial_num_pairs-len(chosen_ref_pairs3))/initial_num_pairs*100))\n",
        "\n",
        "    # Save the pairs to a pickle file\n",
        "\n",
        "\n",
        "    pickle_dump(chosen_sentences3,path,savename+\"_sentences.pkl\")\n",
        "    pickle_dump(chosen_ref_pairs3,path,savename+\"_ref_pairs.pkl\")\n",
        "\n",
        "    return chosen_sentences3,chosen_ref_pairs3\n",
        "\n",
        "\n",
        "def train(epochs,model,optimizer,criterion,train_loader,val_loader,stopper,device,lr_scheduler,eval_period,clip=1.0):\n",
        "    train_losses, val_losses = [],[]\n",
        "    vocab = train_loader.dataset.vocabulary\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data,targets[:,:-1])\n",
        "            loss = criterion(output.view(-1,output.size(-1)),targets[:,1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "            if clip:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
        "            optimizer.step()\n",
        "            if (i+1)%eval_period==0 or i==len(train_loader)-1:\n",
        "                train_losses.append(loss.item())\n",
        "\n",
        "                r_index = random.randint(0, len(data) - 1)\n",
        "                r_target = targets[r_index].view(-1).cpu().detach().numpy()\n",
        "                r_output = output[r_index].argmax(dim=-1).view(-1).cpu().detach().numpy()\n",
        "\n",
        "                # Evaluation\n",
        "                val_losses.append(evaluate(model, criterion, val_loader, device))\n",
        "                print(\"Epoch: {}/{}, Batch: {}/{}:\\n- Train Loss: {:.4f}\".format(epoch+1,epochs,i,len(train_loader),train_losses[-1]))\n",
        "                print(\"- Validation Loss: {:.4f}\\n\".format(val_losses[-1]))\n",
        "                print(\"- Target: \"+\" \".join([vocab.index2word[i] for i in r_target]))\n",
        "                print(\"- Output: \"+\" \".join([vocab.index2word[i] for i in r_output]))\n",
        "\n",
        "        lr_scheduler.step()\n",
        "        if  stopper.check_early_stop(val_losses[-1], model):\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "    return train_losses, val_losses, model, stopper\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, criterion, val_loader, device):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data, targets in val_loader:\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            output = model(data, targets[:, :-1])\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "            val_loss += loss.item()\n",
        "    model.train()\n",
        "    return val_loss / len(val_loader)\n",
        "\n",
        "\n",
        "\n",
        "def train_ga(epochs, model, optimizer, criterion, train_loader, val_loader, stopper, device, lr_scheduler, print_every_n, accumulation_steps, vocab, clip=1.0):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            data, targets = data.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data, targets[:, :-1])\n",
        "            loss = criterion(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient accumulation\n",
        "            if (i + 1) % accumulation_steps == 0:\n",
        "                if clip:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "                optimizer.step()\n",
        "                lr_scheduler.step()\n",
        "                total_loss += loss.item()\n",
        "                steps += 1\n",
        "\n",
        "                if (i + 1) % print_every_n == 0:\n",
        "                    avg_loss = total_loss / steps\n",
        "                    train_losses.append(avg_loss)\n",
        "                    total_loss = 0.0\n",
        "                    steps = 0\n",
        "                    val_losses.append(evaluate(model, criterion, val_loader, device))\n",
        "\n",
        "                    r_index = random.randint(0, len(data) - 1)\n",
        "                    r_target = targets[r_index].view(-1).cpu().detach().numpy()\n",
        "                    r_output = output[r_index].argmax(dim=-1).view(-1).cpu().detach().numpy()\n",
        "                    print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}]\\n- Train Loss: {avg_loss:.4f}\")\n",
        "                    print(f\"- Validation Loss: {val_losses[-1]:.4f}\")\n",
        "                    print(\"- Target: \"+\" \".join([vocab.index2word[i] for i in r_target if i!=0]))\n",
        "                    print(\"- Output: \"+\" \".join([vocab.index2word[i] for i in r_output])+\"\\n\")\n",
        "                    \n",
        "\n",
        "\n",
        "\n",
        "        if  stopper.check_early_stop(val_losses[-1], model):\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    return train_losses, val_losses, model, stopper\n",
        "\n",
        "\n",
        "\n",
        "def inference(model, test_sentence, vocab, device, greedy=True):\n",
        "\n",
        "    line,_ = process_sentence(test_sentence)\n",
        "    line = line + ['<EOS>']\n",
        "    line = torch.tensor([vocab.word2index[word] for word in line]).unsqueeze(0)\n",
        "    target = torch.tensor([vocab.word2index[\"<SOS>\"]]).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        line,target= line.to(device), target.to(device)\n",
        "        for _ in range(20):\n",
        "            output = model(line,target)\n",
        "            if greedy:\n",
        "                next_word_idx = output.argmax(dim=-1)[:,-1].unsqueeze(1)\n",
        "            else:\n",
        "                next_word_idx = torch.multinomial(F.softmax(output,dim=-1)[:,-1],1)\n",
        "            target = torch.cat((target, next_word_idx), dim=1)\n",
        "            if next_word_idx.item() == vocab.word2index[\"<EOS>\"]:\n",
        "                break\n",
        "  \n",
        "        line = line.view(-1).detach().cpu().numpy()\n",
        "        target = target.view(-1).detach().cpu().numpy()\n",
        "        print(\"Input:\")\n",
        "        [print(vocab.index2word[i.item()],end=\" \") for i in line]\n",
        "        print(\"\\nOutput:\")\n",
        "        [print(vocab.index2word[i.item()],end=\" \") for i in target]\n",
        "\n",
        "def train_ga_hf(epochs, model, optimizer, criterion, train_loader, val_loader, device, lr_scheduler, accumulation_steps):\n",
        "    accelerator = Accelerator(gradient_accumulation_steps=accumulation_steps)\n",
        "    model, optimizer, train_loader, lr_scheduler = accelerator.prepare(model, optimizer, train_loader, lr_scheduler)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            with accelerator.accumulate(model):\n",
        "                data, targets = data.to(device), targets.to(device)\n",
        "                output = model(data, targets[:, :-1])\n",
        "                loss = criterion(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "                train_losses.append(loss.item())\n",
        "                accelerator.backward(loss)\n",
        "                optimizer.step()\n",
        "                if lr_scheduler and (i+1) % accumulation_steps==0:\n",
        "                    val_losses.append(evaluate(model, criterion, val_loader, device))\n",
        "                    lr_scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "    return train_losses, val_losses, model\n",
        "\n",
        "\n",
        "def train_with_gradient_accumulation(epochs, model, criterion, optimizer, train_loader, device, val_loader, accumulation_steps, lr_scheduler):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    vocab = train_loader.dataset.vocabulary\n",
        "    step=0\n",
        "    optimizer.zero_grad()\n",
        "    running_loss = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i, (data, targets) in enumerate(train_loader):\n",
        "            model.train()\n",
        "            step += 1\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(data,targets[:,:-1])\n",
        "            loss = criterion(outputs.reshape(-1,outputs.size(-1)), targets[:,1:].reshape(-1))\n",
        "            loss = loss / accumulation_steps\n",
        "            running_loss += loss.item()\n",
        "            loss.backward()\n",
        "\n",
        "            if step % accumulation_steps == 0 or (i + 1 == len(train_loader) and epoch + 1 == epochs):\n",
        "                train_losses.append(running_loss)\n",
        "                running_loss = 0\n",
        "\n",
        "                model.eval()\n",
        "                total_loss = 0  \n",
        "                with torch.no_grad():\n",
        "                    for data, targets in val_loader:\n",
        "                        data = data.to(device)\n",
        "                        targets = targets.to(device)\n",
        "                        outputs = model(data,targets[:,:-1])\n",
        "                        loss = criterion(outputs.reshape(-1,outputs.size(-1)), targets[:,1:].reshape(-1))\n",
        "                        total_loss += loss.item()\n",
        "                    val_losses.append(total_loss / len(val_loader))  \n",
        "                model.train()\n",
        "                                \n",
        "                optimizer.step()\n",
        "                lr_scheduler.step(val_losses[-1])\n",
        "                optimizer.zero_grad()\n",
        "                \n",
        "                random_index = random.randint(0, len(targets)-1)\n",
        "                target_sentence = \" \".join([vocab.index2word[idx.item()] for idx in targets[random_index][1:]])\n",
        "                output_sentence = \" \".join([vocab.index2word[idx.item()] for idx in outputs.argmax(dim=-1)[random_index]])\n",
        "\n",
        "                print(f\"Epoch: {epoch+1}/{epochs}, Batch: {i+1}/{len(train_loader)}\")\n",
        "                print(f\"Train Loss: {train_losses[-1]:.4f}\")\n",
        "                print(f\"Validation Loss: {val_losses[-1]:.4f}\")\n",
        "                print(f\"Random Target Sentence: {target_sentence}\")\n",
        "                print(f\"Random Output Sentence: {output_sentence}\\n\")\n",
        "\n",
        "\n",
        "    return train_losses, val_losses, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cyq1djiEBhE"
      },
      "outputs": [],
      "source": [
        "# Load pairs\n",
        "############################\n",
        "PATH = './MyFiles/'\n",
        "#chosen_sentences,chosen_ref_pairs = create_pairs(path='./MyFiles/',savename=\"result\",max_length=26,word_frequency_discard=10,verbose=True)\n",
        "chosen_sentences = pickle_load(PATH+'result_sentences.pkl')\n",
        "chosen_ref_pairs = pickle_load(PATH+'result_ref_pairs.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dump QA chosen_sentences\n",
        "############################\n",
        "PATH = './MyFiles/'\n",
        "with open(PATH+'qa_chosen_dump.txt', 'w') as qa_dump:\n",
        "    for s in list(chosen_sentences.values()):\n",
        "        s_temp = \" \".join(s)\n",
        "        qa_dump.write(s_temp+\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuT53XoGEBhE"
      },
      "outputs": [],
      "source": [
        "# Pipeline parameters\n",
        "############################\n",
        "\n",
        "# Hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 64\n",
        "accumulation_steps = 1\n",
        "rand_sample_num = 20480 #40960\n",
        "learning_rate = 1e-3\n",
        "d_model = 512 #768\n",
        "encoder_layers = 6\n",
        "decoder_layers = 6\n",
        "feed_forward_dim = 2048\n",
        "nheads = 8\n",
        "dropout_transformer = 0.2\n",
        "dropout_posenconding = 0\n",
        "patience = 3\n",
        "epochs = 2\n",
        "weight_decay = 0\n",
        "\n",
        "\n",
        "\n",
        "# Randomly sample pairs and splits\n",
        "rand_sample_num = min(rand_sample_num,len(chosen_ref_pairs))\n",
        "chosen_ref_pairs = random.sample(chosen_ref_pairs,rand_sample_num)\n",
        "train_size = (int(0.8 * rand_sample_num)//batch_size)*batch_size\n",
        "val_size = (rand_sample_num-train_size)//2\n",
        "test_size = rand_sample_num-train_size-val_size\n",
        "train_pairs, test_pairs, val_pairs = random_split(chosen_ref_pairs,[train_size,val_size,test_size])\n",
        "\n",
        "# Vocabulary, Dataset and Dataloader\n",
        "vocab = Vocabulary(\"English\",chosen_sentences.values())\n",
        "train_dataset = Dataset(vocab,train_pairs,extract_sentences_from_refs(train_pairs, chosen_sentences))\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,collate_fn=lambda batch: collate_fn(batch,vocab.word2index[\"<PAD>\"]))\n",
        "val_dataset = Dataset(vocab,val_pairs,extract_sentences_from_refs(val_pairs, chosen_sentences))\n",
        "val_loader = DataLoader(val_dataset,batch_size=batch_size,shuffle=False,collate_fn=lambda batch: collate_fn(batch,vocab.word2index[\"<PAD>\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dump QA dataset\n",
        "############################\n",
        "PATH = './MyFiles/'\n",
        "with open(PATH+'qa_dataset_dump.txt', 'w') as qa_dump:\n",
        "    for Q,A in train_dataset:\n",
        "        q = \" \".join([vocab.index2word[i.item()] for i in Q])\n",
        "        a = \" \".join([vocab.index2word[i.item()] for i in A])\n",
        "        qa_dump.write(\"Q: \"+q+\"\\nA: \"+a+\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3okvUcaCEBhE",
        "outputId": "f3d603f2-a9ac-4267-ce7c-33214103e3e8"
      },
      "outputs": [],
      "source": [
        "# Pipeline initialization\n",
        "############################\n",
        "\n",
        "# Model, criterion, optimizer and scheduler\n",
        "model = TransformerModel(vocab_size=len(vocab.word2index),d_model=d_model,pad_id=vocab.word2index[\"<PAD>\"],\n",
        "                         encoder_layers=encoder_layers,decoder_layers=decoder_layers,dim_feedforward=feed_forward_dim,\n",
        "                         num_heads=nheads,dropout_transformer=dropout_transformer, dropout_posenconding=dropout_posenconding).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95, verbose=False)\n",
        "stopper = EarlyStopper(patience=patience)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses, val_losses, model = train_with_gradient_accumulation(epochs, model, criterion, optimizer, train_loader, device, val_loader, accumulation_steps, lr_scheduler, print_every_n_batches=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iRgZi-crEBhE",
        "outputId": "230dba3c-e23e-4cd6-d93e-0b2ab33bafe6"
      },
      "outputs": [],
      "source": [
        "# Standard Training\n",
        "############################\n",
        "eval_period = 8\n",
        "train_losses, val_losses, model, stopper = train(epochs,model,optimizer,criterion,train_loader,val_loader,stopper,device,lr_scheduler,eval_period,clip=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "FDI_sH8rEBhF",
        "outputId": "5ad337c0-c5af-4bf5-8b9a-8a9e8d74f54a"
      },
      "outputs": [],
      "source": [
        "# Plot train losses and validation losses\n",
        "plt.plot(loss_hist_train, label='Train Loss')\n",
        "plt.plot(loss_hist_val, label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train Loss vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYykDUCmEBhF"
      },
      "outputs": [],
      "source": [
        "# Checkpointing\n",
        "############################\n",
        "\n",
        "PATH = './MyFiles/'\n",
        "#save_checkpoint = checkpoint(model,optimizer,path=PATH,save_name=\"checkpoint_dicts\")\n",
        "#torch.save(model, PATH+'model.pt')\n",
        "#model_state, optimizer_state = load_checkpoint(PATH+\"checkpoint_dicts.pt\")\n",
        "#pickle_dump(train_losses,PATH,\"train_losses.pkl\")\n",
        "#pickle_dump(val_losses,PATH,\"val_losses.pkl\")\n",
        "\n",
        "model = torch.load(PATH+'model.pt')\n",
        "train_losses = pickle_load(PATH+\"train_losses.pkl\")\n",
        "val_losses = pickle_load(PATH+\"val_losses.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test loss\n",
        "############################\n",
        "\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "\n",
        "testing_loader = DataLoader(Dataset(vocab,test_pairs,extract_sentences_from_refs(test_pairs,chosen_sentences)),batch_size=batch_size,\n",
        "                            shuffle=True,collate_fn=lambda batch: collate_fn(batch,vocab.word2index[\"<PAD>\"]))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in testing_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "        output = model(data, targets[:, :-1])\n",
        "        loss = criterion(output.view(-1, output.size(-1)), targets[:, 1:].contiguous().view(-1))\n",
        "        test_loss += loss.item()\n",
        "\n",
        "test_loss /= len(testing_loader)\n",
        "print(\"Test Loss: {:.4f}\".format(test_loss))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inference\n",
        "############################\n",
        "testing_loader = DataLoader(Dataset(vocab,test_pairs,extract_sentences_from_refs(test_pairs,chosen_sentences)),batch_size=1,\n",
        "                            shuffle=True,collate_fn=lambda batch: collate_fn(batch,vocab.word2index[\"<PAD>\"]))\n",
        "data,_= next(iter(testing_loader))\n",
        "data = \" \".join([vocab.index2word[i] for i in data[0].detach().tolist()[:-1]])\n",
        "inference(model, data, vocab, device, greedy=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Auxiliary calculations\n",
        "############################\n",
        "\n",
        "def prime_factors(n):\n",
        "    i = 2\n",
        "    while i * i <= n:\n",
        "        if n % i == 0:\n",
        "            n /= i\n",
        "            yield i\n",
        "        else:\n",
        "            i += 1\n",
        "    if n > 1:\n",
        "        yield n\n",
        "\n",
        "\n",
        "def prod(iterable):\n",
        "    result = 1\n",
        "    for i in iterable:\n",
        "        result *= i\n",
        "    return result\n",
        "\n",
        "\n",
        "def get_divisors(n):\n",
        "    pf = prime_factors(n)\n",
        "\n",
        "    pf_with_multiplicity = collections.Counter(pf)\n",
        "\n",
        "    powers = [\n",
        "        [factor ** i for i in range(count + 1)]\n",
        "        for factor, count in pf_with_multiplicity.items()\n",
        "    ]\n",
        "\n",
        "    for prime_power_combo in itertools.product(*powers):\n",
        "        yield prod(prime_power_combo)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Number of batches: {}\".format(train_size/batch_size))\n",
        "print(\"Size of batches: {}\".format(batch_size))\n",
        "print(\"Update every {} batches\".format(accumulation_steps))\n",
        "possible_logs_per_epoch = train_size/(batch_size*accumulation_steps)\n",
        "print(\"Number of possible logs per epoch: {}\".format(possible_logs_per_epoch))\n",
        "divisors = list(get_divisors(possible_logs_per_epoch))\n",
        "print(\"Divisors of {}: {}\".format(possible_logs_per_epoch,divisors))\n",
        "print(\"Frequency per epoch: {}\".format((possible_logs_per_epoch/np.array(divisors)).tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient accumulation training\n",
        "############################\n",
        "logs_per_epoch = 16\n",
        "print_every_n = 16 #int(possible_logs_per_epoch/logs_per_epoch)\n",
        "\n",
        "train_losses, val_losses, model, stopper = train_ga(epochs,model,optimizer,criterion,train_loader,val_loader,stopper,device,lr_scheduler,print_every_n,accumulation_steps,vocab,clip=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient accumulation training\n",
        "############################\n",
        "logs_per_epoch = 16\n",
        "print_every_n = int(possible_logs_per_epoch/logs_per_epoch)\n",
        "\n",
        "train_losses2, val_losses2, model, stopper = train_ga(1,model,optimizer,criterion,train_loader,val_loader,stopper,device,lr_scheduler,print_every_n,accumulation_steps,vocab,clip=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot train losses and validation losses\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train Loss vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Checkpointing\n",
        "############################\n",
        "\n",
        "PATH = './MyFiles/'\n",
        "#save_checkpoint = checkpoint(model,optimizer,path=PATH,save_name=\"checkpoint_dicts\")\n",
        "#torch.save(model, PATH+'model.pt')\n",
        "#model_state, optimizer_state = load_checkpoint(PATH+\"checkpoint_dicts.pt\")\n",
        "\n",
        "#pickle_dump(train_losses,PATH,\"train_losses.pkl\")\n",
        "#pickle_dump(val_losses,PATH,\"val_losses.pkl\")\n",
        "\n",
        "model = torch.load(PATH+'model.pt')\n",
        "#train_losses = pickle_load(PATH+\"train_losses.pkl\")\n",
        "#val_losses = pickle_load(PATH+\"val_losses.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_state_dict(model_state)\n",
        "optimizer.load_state_dict(optimizer_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Gradient accumulation hugging face training\n",
        "############################\n",
        "train_losses, val_losses, model = train_ga_hf(epochs, model, optimizer, criterion, train_loader, val_loader, device, lr_scheduler, accumulation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zurtE4HXEBhF"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # !!! Don't change the seed !!!\n",
        "    torch.manual_seed(42)\n",
        "    # !!!!!!\n",
        "\n",
        "    # Download the data\n",
        "\n",
        "    # SAVE and put the code above into a function that you will call if you need to generate something slightly different\n",
        "    PATH = './MyFiles/'\n",
        "    #pairs = create_pairs(path=PATH,savename1=\"pre_pairs\",savename2=\"pairs\",max_length=26,word_frequency_discard=10,verbose=False)\n",
        "    pairs = pickle_load(PATH+'pairs.pkl')\n",
        "\n",
        "    # Training loop (Consider writing a function for this/two separate functions for training and validation)\n",
        "\n",
        "    # Evaluation by feeding the model with one input sentence at a time\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
